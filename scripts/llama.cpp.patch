--- llama-orig.cpp	2023-08-11 07:15:38
+++ llama.cpp	2023-08-11 07:08:07
@@ -3336,6 +3336,12 @@
     if (params.n_gpu_layers > 0) {
         // this allocates all Metal resources and memory buffers
         ctx->ctx_metal = lm_ggml_metal_init(1);
+
+        if (!ctx->ctx_metal) {
+            LLAMA_LOG_ERROR("%s: lm_ggml_metal_init() failed\n", __func__);
+            llama_free(ctx);
+            return NULL;
+        }
 
         void * data_ptr  = NULL;
         size_t data_size = 0;
